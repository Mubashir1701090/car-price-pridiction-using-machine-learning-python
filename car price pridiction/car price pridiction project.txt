import pandas as pd
import numpy as np
from sklearn.model_selection import train_test_split, cross_val_score
from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor
from sklearn.metrics import mean_absolute_error, mean_squared_error
import matplotlib.pyplot as plt
import seaborn as sns

# Load dataset
data = pd.read_csv('car_prices.csv')  # Assuming the dataset is in a CSV file named 'car_prices.csv'

# Data Exploration
print("Dataset Info:")
print(data.info())

print("\nDataset Description:")
print(data.describe())

print("\nDataset Head:")
print(data.head())

# Visualize the distribution of the target variable
plt.figure(figsize=(10, 6))
sns.histplot(data['price'], kde=True)
plt.title('Price Distribution')
plt.show()

# Check for missing values
print("\nMissing Values:")
print(data.isnull().sum())

# Feature Engineering
data['age_of_car'] = pd.Timestamp.now().year - data['manufacture_year']
data = pd.get_dummies(data, columns=['brand', 'model', 'fuel_type', 'transmission'], drop_first=True)

# Split the data into features and target variable
X = data.drop('price', axis=1)
y = data['price']

# Split the data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Model Selection and Training
models = {
    'Random Forest': RandomForestRegressor(random_state=42),
    'Gradient Boosting': GradientBoostingRegressor(random_state=42)
}

# Train and evaluate models
for name, model in models.items():
    model.fit(X_train, y_train)
    y_pred = model.predict(X_test)
    
    mae = mean_absolute_error(y_test, y_pred)
    rmse = np.sqrt(mean_squared_error(y_test, y_pred))
    
    print(f"{name} - MAE: {mae}, RMSE: {rmse}")
    
    # Cross-validation
    scores = cross_val_score(model, X, y, cv=5, scoring='neg_mean_absolute_error')
    print(f"{name} - Cross-validated MAE: {-np.mean(scores)}")

# Pro Tips
def plot_feature_importance(model, X):
    feature_importance = model.feature_importances_
    sorted_idx = np.argsort(feature_importance)
    y_ticks = np.arange(0, len(feature_importance))
    fig, ax = plt.subplots()
    ax.barh(y_ticks, feature_importance[sorted_idx])
    ax.set_yticklabels(np.array(X.columns)[sorted_idx])
    ax.set_yticks(y_ticks)
    ax.set_title("Random Forest Feature Importances")
    plt.show()

# Plot feature importance for Random Forest
plot_feature_importance(models['Random Forest'], X)